`<!DOCTYPE HTML>
<html>

<head>
  <title>Tomasz Trzcinski</title>
  <meta name="description" content="website description" />
  <meta name="keywords" content="website keywords, website keywords" />
  <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
  <link rel="stylesheet" type="text/css" href="style.css" title="style" />
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-62894476-1', 'auto');
  ga('send', 'pageview');

</script>
</head>

<body>
  <div id="main">
    <div id="header">
      <div id="logo">
        <div id="logo_text">
          <!-- class="logo_colour", allows you to change the colour of the text -->
          <h1><a href="index.html">prof. dr hab. inż. Tomasz<span class="logo_colour"> Trzciński</span></a></h1>
          <h2>Profesor, Politechnika Warszawska</h2>
          <h2><a href="index.html">English</a></h2>
        </div>
      </div>
    <div id="site_content">
      <div id="content">
        <h1>Biografia</h1>
        <span class="right"><img src="photo2.jpg" height="220" hspace="5" vspace="5" style="PADDING-LEFT: 15px; PADDING-RIGHT: 5px;"/></span>

        <p align="justify"><b>Prof. dr hab. inż. Tomasz Trzciński, prof. PW</b> <!--w <a href="http://www.grafi.ii.pw.edu.pl/" target="_blank">Zakładzie Grafiki Komputerowej</a> <a href="http://www.ii.pw.edu.pl/" target="_blank">Instytutu Informatyki</a>
<a href="http://www.pw.edu.pl">Politechnice Warszawskiej</a> od 2015 r., gdzie --> kieruje pracami zespołu zajmującego się widzeniem maszynowym <a href="http://cvlab.ii.pw.edu.pl" target="_blank">CVLab</a>. Kieruje pracami grupy widzenia maszynowego w <a href="https://ideas-ncbr.pl/">IDEAS NCBR</a>, polskim centrum badawczym w obszarze sztucznej inteligencji. Tytuł naukowy profesora uzyskał w lipcu 2024 r., stopień doktora habilitowanego w 2020 r., doktora w zakresie wizji maszynowej na <a href="http://www.epfl.ch" target="_blank">École Polytechnique Fédérale de Lausanne</a> w 2014 r., a podwójny dyplom magisterski na <a href="http://www.upc.edu" target="_blank">Universitat Politècnica de Catalunya</a> oraz <a href="http://www.polito.it" target="_blank">Politecnico di Torino</a>. 
W latach 2020-2023 był również członkiem zespołu uczenia maszynowego <a href="https://gmum.net/" target="_blank">GMUM</a> na <a href="https://www.uj.edu.pl/" target="_blank">Uniwersytecie Jagiellońskim</a>. Odbył staże naukowe na <a href="http://stanford.edu" target="_blank">Uniwersytecie Stanforda</a> w 2017 r. oraz na <a href="https://www.ntu.edu.sg/" target="_blank">Nanyang Technological University</a> w 2019 r.
Jest recenzentem prac publikowanych w czasopismach TPAMI, IJCV, CVIU, TIP i TMM, oraz członkiem komitetów organizacyjnych konferencji, m.in. CVPR, ICCV i ICML. Pracował w <a href="http://www.google.com" target="_blank">Google</a> w 2013 r., <a href="http://www.qualcomm.com/invention/research" target="_blank">Qualcomm</a> w 2012 r. oraz w <a href="http://www.tid.es" target="_blank">Telefónice</a> w 2010 r.  Pełni funkcję Senior Member w <a href="https://www.ieee.org/" target="_blank">IEEE</a>, jest członkiem <a href="https://ellis.eu/members">ELLIS Society</a> i dyrektorem jej <a href="https://ellis.eu/units/warsaw">warszawskiego oddziału</a>, oraz członkiem ALICE Collaboration w <a href="https://home.web.cern.ch/" target="_blank">CERN</a>, jest ekspertem <a href="https://www.ncn.gov.pl" target="_blank">Narodowego Centrum Nauki</a> i <a href="http://fnp.org.pl	" target="_blank">Fundacji na rzecz Nauki Polskiej</a>. Jest współwłaścicielem oraz Chief Scientist w <a href="http://www.tooploox.com" target="_blank">Tooploox</a>, a także współzałożycielem startupu technologicznego <a href="http://comixify.ai">Comixify</a>, wykorzystującego metody sztucznej inteligencji do edycji wideo.</p>
        <p><b>Zainteresowania naukowe:</b>
        <i>widzenie maszynowe</i> (symultaniczna lokalizacja i mapowanie, wyszukiwanie wizualne), <i>wydajne uczenie maszynowe</i> (głębokie sieci neuronowe, modele generatywne, uczenie ciągłe, warunkowanie obliczeń), <i>uczenie reprezentacji</i> (deskryptory binarne).
<!--<p><b>Aktualnie poszukuję osób chętnych do wzięcia udziału w projekcie we współpracy z <a href="http://www.google.com">Google</a>. Szczegóły <a href="google_project_description.pdf" taget="_blank">tutaj</a>.</b>-->

<!--	<p><b>Od 22.11.2019 do 30.12.2019 będę realizował pobyt naukowy na <a href="http://www.ntu.edu.sg/">Nanyang Technological University w Singapurze</a>, w związku z czym nie będę dostępny na uczelni.</b></p>-->

<!--        <p><b>Jestem organizatorem Focused Session <a href="http://mrw2018.org/ursi2018/focused-sessions-ursi/#fs1-2" target="_blank">Recent advancements in computer vision and image processing with special focus on deep learning methods</a> w ramach konferencji MRV. Zapraszam do <a href="http://mrw2018.org/instructions-for-authors/" target="_blank">złożenia publikacji (URSI)</a>.</b></p>-->

<!--   <p><b>Oferta pracy na stanowisku post-doc w 3-letnim projekcie NCN <i>Głębokie generatywne spojrzenie na uczenie ciągłe</i>. Aplikacje do 30 czerwca 2021. Więcej informacji dostępnych <a href="https://ncn.gov.pl/baza-ofert/?akcja=wyswietl&id=186315">tutaj</a>.</b></p>-->

        <h1>Kontakt</h1>
        <p><b>adres</b>: ul. Nowowiejska 15/19, 00-665 Warszawa, Poland<br>
        <b>email</b>: <a href="mailto:tomasz.trzcinski@pw.edu.pl">tomasz.trzcinski@pw.edu.pl</a><br>
        <b>tel</b>: +48 22 234 7650<br>
        <b>konsultacje</b>: po wcześniejszym uzgodnieniu drogą elektroniczną</p>

        <h2>Publikacje</h2>

<!--        <h4><b>Nowe:</b></h4>
        <ul>
         </ul>-->

        <h4><b>Wybrane artykuły w czasopismach:</b></h4>
        <ul>
<li>B. Wójcik, M. Przewięźlikowski, F. Szatkowski, M. Wołczyk, K. Bałazy, B. Krzepkowski, I. Podolak, J. Tabor, M. Śmieja, T. Trzciński. <b>Zero Time Waste in Pre-trained Early Exit Neural Networks</b>, Neural Networks, Vol. 168, p. 580-601, 2023. <a href="https://www.sciencedirect.com/science/article/pii/S0893608023005555" target="_blank">pdf</a></li>
<li>M. Zamorski, M. Stypułkowski, K. Karanowski, T. Trzcinski, M. Zieba. <b>Continual learning on 3D point clouds with random compressed rehearsal</b>, Computer Vision and Image Understanding, 2023. <a href="https://www.sciencedirect.com/science/article/abs/pii/S1077314223000012" target="_blank">pdf</a></li>
	<li>J. Komorowski, M. Wysoczanska, T. Trzcinski. <b>EgoNN: Egocentric Neural Network for Point Cloud Based 6DoF Relocalization at the City Scale</b>, IEEE Robotics and Automation Letters, 2021. <a href="https://arxiv.org/abs/2110.12486">arXiv</a></li>
	<li>P. Spurek, M. Zięba, J. Tabor, T. Trzcinski. <b>General hypernetwork framework for
creating 3D point clouds</b>, IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI), 2021.</li>
	<li>M. Stypułkowski, K. Kania, M. Zamorski, M. Zięba, T. Trzcinski, J. Chorowski. <b>Representing point clouds with generative conditional invertible flow networks</b>, Pattern Recognition Letters, Vol. 150, p. 26-32, 2021. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865521002294?via%3Dihub">pdf</a></li>
          <li>K. Deja, J. Dubinski, P. Nowak, S. Wenzel, P. Spurek, T. Trzcinski. <b>End-to-end Sinkhorn Autoencoder with Noise Generator</b>. IEEE Access, 2021. <a href="https://ieeexplore.ieee.org/document/9311504">pdf</a></li>
          <li>M. Zamorski, M. Zieba, P. Klukowski, R. Nowak, K. Kurach, W. Stokowiec, T. Trzcinski. <b> Adversarial autoencoders for compact representations of 3D point clouds</b>, Computer Vision and Image Understanding, 2020. <a href="https://arxiv.org/abs/1811.07605" target="_blank">arXiv</a></li>
          <li>I. Tautkute, T. Trzcinski, A. Skorupa, L. Brocki, K. Marasek. <b>DeepStyle: Multimodal Search Engine for Fashion and Interior Design</b>. IEEE Access, Vol. 6, Nr. 1, p. 84613-84628, 2019. <a href="https://ieeexplore.ieee.org/document/8737943">pdf</a></li>
        <!-- <li>M. Pesko, A. Svystun, P. Andruszkiewicz, P. Rokita, T. Trzcinski. <b>Comixify: Transform video into comics</b>, Fundamenta Informaticae, Vol. 168, nr 2-4, p. 311-333, 2019. <a href="https://arxiv.org/abs/1812.03473" target="_blank">arXiv</a> <a href="http://comixify.ii.pw.edu.pl" target="_blank">demo</a> <a href="https://github.com/maciej3031/comixify" target="_blank">code</a></li>
          <li>I. Tautkute, T. Trzcinski. <b>Classifying and Visualizing Emotions with EmotionalDAN</b>, Fundamenta Informaticae, Vol. 168, Nr. 2-4, p. 269-285, 2019. <a href="https://arxiv.org/abs/1810.10529" target="_blank">arXiv</a></li>-->
          <li>M. Komorowski, T. Trzcinski. <b>Random Binary Search Trees for approximate nearest neighbour search in binary spaces</b>, Applied Soft Computing, Vol. 79, p. 87-93, 2019. <a href="https://doi.org/10.1016/j.asoc.2019.03.031" target="_blank">official version</a></li>
          <li>A. Bielski, T. Trzcinski. <b>Understanding Multimodal Popularity Prediction of Social Media Videos with Self-Attention</b>. IEEE Access, Vol. 6, Nr. 1, p. 74277-74287, 2018. <a href="https://ieeexplore.ieee.org/document/8558491">pdf</a></li>
          <li>T. Trzcinski, P. Rokita. <b>Predicting popularity of online videos using Support Vector Regression</b>. IEEE Trans. Multimedia (TMM). Vol. 19, Nr. 11, p. 2561-2570, 2017. <a href="https://arxiv.org/abs/1510.06223" target="_blank">arXiv</a></li>
         <li>T. Trzcinski, M. Christoudias, V. Lepetit. <b>Learning Image Descriptors with Boosting</b>. IEEE
             Trans. Pattern Analysis and Machine Intelligence (PAMI). Vol. 37, Nr. 3, pp. 597-610, 2015. <a href="http://infoscience.epfl.ch/record/186698/files/Trzcinski_PAMI_preprint.pdf?version=2" target="_blank">pdf</a></li>
          <li>B. Fan, Q. Kong, T. Trzcinski, Z. Wang, C. Pan, P. Fua. <b>Receptive Fields Selection for Binary
             Feature Description</b>. IEEE Trans. Image Processing (TIP). Vol. 23, Nr. 6, pp. 2583-2595, 2014. <a href="http://dx.doi.org/10.1109/TIP.2014.2317981" target="_blank">official version</a></li>
        <li>T. Trzcinski, V. Lepetit, P. Fua. <b>Thick Boundaries in Binary Space and their Influence on
             Nearest-Neighbor Search</b>. Pattern Recognition Letters (PRL). Vol. 33, pp. 2173-2180, 2012. <a href="http://infoscience.epfl.ch/record/180688/files/prl12_1.pdf?version=1" target="_blank">pdf</a>, <a href="http://infoscience.epfl.ch/record/180688/files/binann_1.0.tar.gz?version=1">code</a></li>
         <li>M. Calonder, V. Lepetit, M. Ozuysal, T. Trzcinski, C. Strecha, P. Fua. <b>BRIEF: Computing
             a local binary descriptor very fast</b>. IEEE Trans. Pattern Analysis and Machine Intelligence
             (PAMI). Vol. 34, Nr. 7, pp. 1281 - 1298, 2012. <a href="http://infoscience.epfl.ch/record/167678/files/top.pdf" target="_blank">pdf</a></li>
        </ul>
        <h4><b>Wybrane publikacje konferencyjne:</b></h4>
        <ul>

<li>A. Pardyl, M. Wronka, M. Wołczyk, K. Adamczewski, T. Trzciński, B. Zieliński. <b>AdaGlimpse: Active Visual Exploration with Arbitrary Glimpse Position and Scale.</b> European Conference on Computer Vision (ECCV), 2024. <a href="https://arxiv.org/abs/2404.03482" target="_target">arXiv</a></li>
<li>M. Wysoczańska, O. Siméoni, M. Ramamonjisoa, A. Bursuc, T. Trzciński, P. Pérez. <b>CLIP-DINOiser: Teaching CLIP a few DINO tricks.</b> European Conference on Computer Vision (ECCV), 2024. <a href="https://arxiv.org/abs/2312.12359" target="_target">arXiv</a></li>
<li>D. Marczak, S. Cygert, T. Trzciński, B. Twardowski. <b>Revisiting Supervision for Continual Representation Learning.</b> European Conference on Computer Vision (ECCV), 2024. <a href="https://arxiv.org/abs/2311.13321" target="_target">arXiv</a></li>
<li>G. Rypeść, D. Marczak, S. Cygert, T. Trzciński, B. Twardowski. <b>CAMP: Category Adaptation Meets Projected Distillation in Generalized Continual Category Discovery.</b> European Conference on Computer Vision (ECCV), 2024. <a href="https://arxiv.org/abs/2308.12112" target="_target">arXiv</a></li>
<li>D. Marczak, B. Twardowski, T. Trzciński, S. Cygert <b>MagMax: Leveraging Model Merging for Seamless Continual Learning.</b> European Conference on Computer Vision (ECCV), 2024. <a href="https://arxiv.org/abs/2407.06322" target="_target">arXiv</a></li>
<li>G. Rypeść, S. Cygert, V. Khan, T. Trzciński, B. Zieliński, B. Twardowski. <b>Divide and not forget: Ensemble of selectively trained experts in Continual Learning
</b>, International Conference on Learning Representations (ICLR), 2024. <a href="https://arxiv.org/abs/2401.10191" target="_target">arXiv</a></li>
<li>W. Masarczyk, M. Ostaszewski, E. Imani, R. Pascanu, P. Miłoś, T. Trzcinski. <b>The Tunnel Effect: Building Data Representations in Deep Neural Networks</b>, Neural Information Processing
Systems (NeurIPS), 2023.  <a href="https://arxiv.org/abs/2305.19753" target="_target">arXiv</a></li>
<li>J. Dubiński, S. Pawlak, F. Boenisch, T. Trzcinski, A. Dziedzic. <b>Bucks for Buckets (B4B):
Active Defenses Against Stealing Encoders</b>, Neural Information Processing Systems (NeurIPS),
2023.</li>
	  <li>M. Grzeszczyk, S. Plotka, B. Rebizant, K. Kosinska-Kaczynska, M. Lipa, R. Brawura-Biskupski-Samaha, P. Korzeniowski, T. Trzciński, A. Sitek. <b>TabAttention: Learning Attention Conditionally on Tabular Data.</b> International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2023.</li>
         <li>A. Pardyl, G. Rypeść, G. Kurzejamski, B. Zieliński, T. Trzciński. <b>Active Visual Exploration Based on Attention-Map Entropy</b>, International Joint Conference on Artificial Intelligence (IJCAI), 2023. <a href="https://arxiv.org/abs/2303.06457" target="_target">arXiv</a></li>
<li>K. Kania, S. J. Garbin, A. Tagliasacchi, V. Estellers, K. Moo Yi, T. Trzcinski, J. Valentin, M. Kowalski. <b>BlendFields: Few-Shot Example-Driven Facial Modeling</b>. Computer Vision and Pattern Recognition (CVPR), 2023. <a href="https://arxiv.org/abs/2305.07514" target="_target">arXiv</a></li>
<li>K. Deja, A. Kuzina, T. Trzcinski, J. Tomczak. <b>On Analyzing Generative and Denoising Capabilities of Diffusion-based Deep Generative Models</b>, Neural Information Processing Systems (NeurIPS), 2022. <a href="https://arxiv.org/abs/2206.00070" target="_target">arXiv</a></li>
<li>P. Lorek, R. Nowak, T. Trzcinski, M. Zieba. <b>FlowHMM: Flow-based continuous hidden Markov models</b>, Neural Information Processing Systems (NeurIPS), 2022.</li>
	<li>P. Spurek, A. Kasymov, M. Mazur, D. Janik, S. Tadeja, Ł. Struski, J. Tabor, T. Trzcinski. <b>HyperPocket: Generative Point Cloud Completion</b>, International Conference on Intelligent Robots and Systems (IROS), 2022. <a href="https://arxiv.org/abs/2102.05973" target="_target">arXiv</a></li>
	 <li>M. Wołczyk, K. Piczak, B. Wójcik, Ł. Pustelnik, P. Morawiecki, J. Tabor, T. Trzcinski, P. Spurek. <b>Continual Learning with Guarantees via Weight Interval Constraints</b>, International Conference on Machine Learning (ICML), 2022. <a href="https://arxiv.org/abs/2206.07996" target="_blank">arXiv</a></li>
	  <li>S. Plotka, M. Grzeszczyk, R. Samaha, P. Gutaj, M. Lipa, T. Trzciński, A. Sitek. <b>BabyNet: Residual Transformer Module for Birth Weight Prediction on Fetal Ultrasound Video.</b> International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2022. <a href="https://arxiv.org/abs/2205.09382" target="_blank">arXiv</a></li>
	  <li>K. Deja, P. Wawrzynski, W. Masarczyk, D. Marczak, T. Trzcinski. <b>Multiband VAE: Latent Space Alignment for Knowledge Consolidation in Continual Learning</b>. International Joint Conference on Artificial Intelligence (IJCAI), 2022. <a href="https://arxiv.org/abs/2106.12196" target="_blank">arXiv</a></li>
          <li>K. Kania, K. Moo Yi, M. Kowalski, T. Trzcinski, A. Tagliasacchi. <b>CoNeRF: Controllable Neural Radiance Fields</b>. Computer Vision and Pattern Recognition (CVPR), 2022. <a href="https://arxiv.org/abs/2112.01983" target="_blank">arXiv</a></li>
         <li>M. Wolczyk, B. Wójcik, K. Bałazy, I. Podolak, J. Tabor, M. Śmieja, T. Trzcinski. <b>Zero Time Waste: Recycling Predictions in Early Exit Neural Networks</b>, Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2106.05409" target="_blank">arXiv</a></li>
         <li>M. Sendera, J. Tabor, A. Nowak, A. Bedychaj, M. Patacchiola, T. Trzcinski, P. Spurek, M. Zieba. <b>Non-Gaussian Gaussian Processes for Few-Shot Regression</b>, Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2110.13561" target="_blank">arXiv</a></li>

<!--        <li>W. Masarczyk, K. Deja, T. Trzcinski. <b>On robustness of generative representations against catastrophic forgetting</b>, International Conference on Neural Information Processing (ICONIP), 2021. <a href="https://arxiv.org/abs/2109.01844" target="_blank">arXiv</a></li>
         <li>S. Płotka, T. Włodarczyk, A. Klasa, M. Lipa, A. Sitek, T. Trzciński. <b>FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements</b>, International Conference on Neural Information Processing (ICONIP), 2021. <a href="https://arxiv.org/abs/2107.06943" target="_blank">arXiv</a></li>
         <li>I. Tautkute-Rustecka, T. Trzcinski. <b>SynthTriplet GAN: Synthetic Query Expansion for Multimodal Retrieval</b>, International Conference on Neural Information Processing (ICONIP), 2021.</li>
         <li>J. Komorowski, M. Wysoczańska, T. Trzcinski. <b>Large-Scale Topological Radar Localization Using Learned Descriptors</b>, International Conference on Neural Information Processing (ICONIP), 2021. <a href="https://arxiv.org/abs/2110.03081" target="_blank">arXiv</a></li></li>
         <li>M. Sadowski, K. Piczak, P. Spurek, T. Trzcinski. <b>Continual Learning of 3D Point Cloud Generators</b>, International Conference on Neural Information Processing (ICONIP), 2021.</li>-->

         <li>D. Basaj, W. Oleszkiewicz, I. Sieradzki, M. Górszczak, B. Rychalska, T. Trzcinski, B. Zielinski. <b>Explaining Self-Supervised Image Representations with Visual Probing</b>, International Joint Conference on Artificial Intelligence (IJCAI), 2021. <a href="https://www.ijcai.org/proceedings/2021/82" target="_blank">pdf</a></li>
<!--          <li>K. Deja, P. Wawrzyński, D. Marczak, W. Masarczyk, T. Trzcinski. <b>BinPlay: A Binary Latent Autoencoder for Generative Replay Continual Learning</b>, International Joint Conference on Neural Networks (IJCNN), 2021. <a href="https://arxiv.org/abs/2011.14960" target="_blank">arXiv</a></li>
        <li>J. Komorowski, M. Wysoczanska, T. Trzcinski. <b>MinkLoc++: Lidar and Monocular Image Fusion for Place Recognition</b>, International Joint Conference on Neural Networks (IJCNN), 2021. <a href="https://arxiv.org/abs/2104.05327" target="_blank">arXiv</a></li>-->
         <li>P. Spurek, S. Winczowski, J. Tabor, M. Zamorski, M. Zięba, T. Trzcinski. <b>Hypernetwork approach to generating point clouds</b>, International Conference on Machine Learning (ICML), 2020. <a href="https://arxiv.org/abs/2003.00802" target="_blank">arXiv</a></li>
         <li>M. Koperski, T. Konopczyński, P. Semberecki, R. Nowak, T. Trzcinski. <b>Plugin Networks for Inference under Partial Evidence</b>, IEEE Workshop on Applications of Computer Vision (WACV), 2020. <a href="https://arxiv.org/abs/1901.00326" target="_blank">arXiv</a></li>
         <!--<li>W. Oleszkiewicz, S. Jastrzębski, T. Makino, T. Trzciński, L. Moy, K. Cho, L. Heacock, K. J. Geras. <b>Understanding the robustness of deep neural network classifiers for breast cancer screening</b>, International Conference on Learning Representations (ICLR), AI for Affordable Healthcare Workshop, 2020. <a href="https://arxiv.org/abs/2003.10041" target="_blank">arXiv</a></li>
         <li>G. Kurzejamski, J. Komorowski, L. Dabala, K. Czarnota, S. Lynen, T. Trzcinski. <b>SuperNCN: Neighbourhood consensus network for robust outdoor scenes matching</b>, Advanced Concepts for Intelligent Vision Systems (ACIVS), 2020. <a href="https://arxiv.org/abs/1912.04627" target="_blank">arXiv</a></li>-->
         <li>M. Zieba, P. Semberecki, T. El-Gaaly, T. Trzcinski. <b>BinGAN: Learning Compact Binary Descriptors with a Regularized GAN</b>. Neural Information Processing Systems (NeurIPS), 2018. <a href="https://arxiv.org/abs/1806.06778" target="_blank">arXiv</a></li>
         <li>N. Kapinski, J. Zielinski, B. Borucki, T. Trzcinski, B. Ciszkowska-Lyson, K. Nowinski. <b>Estimating Achilles tendon healing progress with convolutional neural networks</b>. International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2018. <a href="https://arxiv.org/abs/1806.05091" target="_blank">arXiv</a></li>
<!--         <li>W. Oleszkiewicz, P. Kairouz, K. Piczak, R. Rajagopal, T. Trzcinski. <b>Siamese Generative Adversarial Privatizer for Biometric Data</b>. Asian Conference on Computer Vision (ACCV), 2018. <a href="https://arxiv.org/abs/1804.08757" target="_blank">arXiv</a></li>-->
            <!--      <li>T. Trzcinski, J. Komorowski, L. Dabala, K. Czarnota, G. Kurzejamski, S. Lynen. <b>SConE: Siamese Constellation Embedding Descriptor for Image Matching</b>. European Conference on Computer Vision (ECCV), Workshop on 3D Reconstruction in the Wild, 2018. <a href="https://arxiv.org/abs/1809.11054" target="_blank">arXiv</a></li>
         <li>J. Komorowski, K. Czarnota, T.Trzcinski, L. Dabala, S. Lynen. <b>Interest point detectors stability evaluation on ApolloScape dataset</b>. European Conference on Computer Vision (ECCV), Workshop on ApolloScape: Vision-based Navigation for Autonomous Driving, 2018. <a href="https://arxiv.org/abs/1809.11039" target="_blank">arXiv</a></li>
         <li>A. Bielski, T. Trzcinski. <b>Pay Attention to Virality: understanding popularity of social media videos with the attention mechanism</b>. Computer Vision and Pattern Recognition (CVPR), Understanding Subjective Attributes of Data Workshop, 2018 (<b>oral presentation</b>). <a href="https://arxiv.org/abs/1804.09949" target="_blank">arXiv</a></li>
         <li>I. Tautkute, T. Trzcinski, A. Bielski. <b>I Know How You Feel: Emotion Recognition with Facial Landmarks</b>. Computer Vision and Pattern Recognition (CVPR), Women in Computer Vision Workshop, 2018. <a href="https://arxiv.org/abs/1805.00326" target="_blank">arXiv</a></li>
<li>J. Komorowski, T. Trzcinski. <b>Aggregation of binary feature descriptors for compact scene model representation in large scale structure-from-motion applications</b>. International Conference on Computer Vision and Graphics (ICCVG), 2018. <a href="https://arxiv.org/abs/1809.11062" target="_blank">arXiv</a></li>
         <li>A. Slucki, T. Trzcinski, A. Bielski, P. Cyrta. <b>Extracting textual overlays from social media videos using neural networks</b>. International Conference on Computer Vision and Graphics (ICCVG), 2018. <a href="https://arxiv.org/abs/1804.10687" target="_blank">arXiv</a></li>-->
<!--         <li>T. Trzcinski, M. Glinka, Ł. Graczykowski for the ALICE Collaboration. <b>Using Random Forest Classifier for particle identification in the ALICE Experiment</b>. Conference on Information Technology, Systems Research and Computational Physics (ITSRCP), 2018. <a href="papers/ITSRCP_9.pdf" target="_blank">pdf</a></li>
         <li>K. Deja, T. Trzcinski, Ł. Graczykowski for the ALICE Collaboration. <b>Generative Models for Fast Cluster Simulations in the TPC for the ALICE Experiment</b>. Conference on Information Technology, Systems Research and Computational Physics (ITSRCP), 2018. <a href="papers/ITSRCP_8.pdf" target="_blank">pdf</a></li>-->
         <!--<li>P. Nowakowski, J. Myrcha, T. Trzcinski, Ł. Graczykowski, P. Rokita for the ALICE Collaboration. <b>Comparison of 3D graphics engines for particle track visualization in the ALICE Experiment</b>. Conference on Information Technology, Systems Research and Computational Physics (ITSRCP), 2018. <a href="papers/ITSRCP_28.pdf" target="_blank">pdf</a></li>
         <li>I. Tautkute, T. Trzcinski, A. Bielski. <b>Recognizing Emotions with EmotionalDAN</b>. International Symposium on Computational Modeling of Objects Presented in Images (CompIMAGE), 2018.</li>
         <li>T. Trzcinski, A. Bielski, P. Cyrta, M. Zak. <b>SocialML: machine learning for social media video creators</b>. Neural Information Processing Systems (NIPS) Workshop on Machine Learning for Creativity and Design, 2017. <a href="https://arxiv.org/abs/1802.02204" target="_blank">arXiv</a></li>-->
         <!--<li>T. Trzcinski, P. Andruszkiewicz, T. Bocheński, P. Rokita. <b>Recurrent Neural Networks for Online Video Popularity Prediction</b>. International Symposium on Methodologies for Intelligent Systems (ISMIS), 2017. <a href="https://arxiv.org/abs/1707.06807" target="_blank">arXiv</a></li>
         <li>W. Stokowiec, T. Trzcinski, K. Wolk, K. Marasek, P. Rokita. <b>Shallow reading with Deep Learning: Predicting popularity of online content using only its title</b>. International Symposium on Methodologies for Intelligent Systems (ISMIS), 2017. <a href="https://arxiv.org/abs/1707.06806" target="_blank">arXiv</a></li>-->
         <li>M. Kowalski, J. Naruniec, T. Trzcinski. <b>Deep Alignment Network: A convolutional neural network for robust face alignment</b>. Computer Vision and Pattern Recognition (CVPR), Face Detection in the Wild Workshop, 2017. <a href="https://arxiv.org/abs/1706.01789" target="_blank">arXiv</a></li>
         <!--<li>J. Myrcha, T. Trzcinski, P. Rokita. <b>Virtual Reality Visualization Algorithms for the ALICE High Energy Physics Experiment on the LHC at CERN</b>. IEEE-SPIE Joint Symposium on Photonics, Web Engineering, Electronics for Astronomy and High Energy Physics Experiments (WILGA), 2017. <a href="papers/wilga_2017.pdf" target="_blank">pdf</a></li>
         <li>I. Tautkute, A. Możejko, W. Stokowiec, T. Trzcinski, Ł. Brocki, K. Marasek. <b>What Looks Good with my Sofa: Multimodal Search Engine for Interior Design</b>. FEDCSIS Conference on Multimedia, Interaction, Design and Innovation (MIDI), 2017. <a href="https://arxiv.org/abs/1707.06907" target="_blank">arXiv</a></li>
         <li> M. Suchecki, T. Trzcinski. <b>Understanding aesthetics in photography using deep convolutional neural networks</b>. International Conference on Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA), 2017. <a href="https://arxiv.org/abs/1707.08985" target="_blank">arXiv</a></li>
         <li> P. Cyrta, T. Trzcinski, W. Stokowiec. <b>Speaker Diarization using Deep Recurrent Convolutional Neural Networks for Speaker Embeddings</b>. International Conference on Information Systems Architecture and Technology (ISAT), 2017. <a href="https://arxiv.org/abs/1708.02840" target="_blank">arXiv</a></li>
         <li> J. Komorowski, T. Trzcinski. <b>Evaluation of Hashing Methods Performance on Binary Feature Descriptors</b>. International Conference on Processing and Communications (IPC), 2017. <a href="https://arxiv.org/abs/1707.06825" target="_blank">arXiv</a></li>
         <li> M. Komorowski, T. Trzcinski. <b>Random Binary Trees for Approximate Nearest Neighbour Search in Binary Space</b>. International Conference on Pattern Recognition and Machine Intelligence (PReMI), 2017. <a href="https://arxiv.org/abs/1708.02976" target="_blank">arXiv</a></li>-->
          <li>T. Trzcinski, M. Christoudias, P. Fua, V. Lepetit. <b>Boosting Binary Keypoint Descriptors</b>.
             Computer Vision and Pattern Recognition (CVPR), 2013. <a href="http://infoscience.epfl.ch/record/186246/files/top.pdf?version=1" target="_blank">pdf</a>, <a href="http://infoscience.epfl.ch/record/186246/files/boostDesc_1.0.tar.gz?version=1">code</a></li>
         <li>T. Trzcinski, M. Christoudias, V. Lepetit, P. Fua. <b>Learning Image Descriptors with the
             Boosting-Trick</b>. Neural Information Processing Systems (NIPS), 2012. <a href="http://infoscience.epfl.ch/record/183635/files/top.pdf?version=1" target="_blank">pdf</a></li>
         <li>T. Trzcinski, V. Lepetit. <b>Efficient Discriminative Projections for Compact Binary Descriptors</b>.
             European Conference on Computer Vision (ECCV), 2012. <a href="http://infoscience.epfl.ch/record/183634/files/eccv12.pdf?version=1" target="_blank">pdf</a>, <a href="http://infoscience.epfl.ch/record/183634/files/dbrief_demo_1.1.tar.gz?version=1">code</a></li>
<!--         <li>D. Marimon, T. Adamek, A. Bonnin, T. Trzcinski. <b>Enhancing global positioning by image
             recognition</b>. International Symposium on Mixed and Augmented Reality (ISMAR), Workshop on Enabling Large-Scale Outdoor Mixed Reality and Augmented Reality, 2011. <a href="http://infoscience.epfl.ch/record/183627/files/Marimon11.pdf " target="_blank">pdf</a></li>-->
        </ul>
        
        <h4><b>Inne:</b></h4>
        <ul>
          <li>T. Trzcinski. <b>Learning and Matching Binary Local Feature Descriptors</b>. Ph.D. Thesis, EPFL, n° 6226 (2014). <a href="http://infoscience.epfl.ch/record/200862/files/EPFL_TH6226.pdf" target="_blank">pdf</a></li>
          <li>T. Trzcinski. <b>Towards Precise Outdoor Localisation Based on Image Recognition</b>. M.Sc. Thesis, Universitat Politècnica de Catalunya, Politecnico di Torino (2010). <a href="papers/msc_thesis.pdf" target="_blank">pdf</a></li>
        </ul>

        Profil <a href="https://scholar.google.pl/citations?user=bJMRBFoAAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a>

      <h2>Artykuły prasowe</h2>
        <span class="center"><a href="https://www.dropbox.com/s/m6vf1sjjnhnmsyq/RDC_Swiatek_Piatek_Tomasz_Trzcinski.mp3?dl=0"><img src="rdc.jpg" height="100" hspace="5" vspace="5" style="PADDING-LEFT: 5px; PADDING-RIGHT: 5px;"/></a>
        <a href="https://www.dropbox.com/s/4gvgvxz5z3d5v64/RADIO_CAMPUS_2017-01-30.0110_neweurope100.mp3?dl=0"><img src="kampus.png" height="110" hspace="5" vspace="5" style="PADDING-LEFT: 5px; PADDING-RIGHT: 5px;"/></a>
        <a href="http://innpoland.pl/136283,to-prawdziwy-hit-polskiej-nauki-badacze-z-politechniki-powiedza-ci-co-zrobic-by-byc-popularnym-w-internecie"><img src="inn2.png" height="60" hspace="5" vspace="5" style="PADDING-LEFT: 5px; PADDING-RIGHT: 5px; PADDING-BOTTOM: 25px"/></a>
        <a href="https://www.dropbox.com/s/t2ecqbi8ug6p3mb/TROJKA_2017-07-17_trzcinski.mp3?dl=0"><img src="trojka.jpg" height="100" hspace="5" vspace="5" style="PADDING-LEFT: 15px; PADDING-RIGHT: 15px; PADDING-BOTTOM: 5px;"/></a>
        <a href="http://naukawpolsce.pap.pl/aktualnosci/news,414996,memy-i-virale-co-zyska-popularnosc-w-internecie-przewidzi-to-algorytm.html"><img src="pap.png" height="70" hspace="5" vspace="5" style="PADDING-LEFT: 15px; PADDING-RIGHT: 15px; PADDING-BOTTOM: 20px;"/></a>
        <a href="http://innpoland.pl/136051,dlaczego-w-jedne-linki-klikamy-a-w-drugie-nie-badaja-to-warszawscy-naukowcy-a-firmy-juz-ustawiaja-sie-w-kolejce"><img src="inn.png" height="60" hspace="5" vspace="5" style="PADDING-LEFT: 5px; PADDING-RIGHT: 5px;"/></a>
        <a href="http://www.polskieradio.pl/130/5925/Artykul/1870804,Czy-to-sie-poniesie-w-internecie"><img src="polskieradio24.jpg" height="100" hspace="5" vspace="5" style="PADDING-LEFT: 5px; PADDING-RIGHT: 5px;"/></a>
        <a href="https://www.pw.edu.pl/Badania-i-nauka/Badania-Innowacje-Technologie-BIT-PW/Jak-przewidziec-popularnosc-tresci-w-Internecie"><img src="pw.png" height="50" hspace="5" vspace="5" style="PADDING-LEFT: 15px; PADDING-RIGHT: 5px; PADDING-BOTTOM: 15px;"/></a>
        <a href="http://www.elka.pw.edu.pl/Aktualnosci/Wydarzenia/Dr-Tomasz-Trzcinski-na-Stanford-University"><img src="elka.jpg" height="100" hspace="5" vspace="5" style="PADDING-LEFT: 5px; PADDING-RIGHT: 5px;"/></a>
        <a href="https://engadget.com/2018/12/12/comixify-algorithm-turns-video-to-comics/"><img src="engadget.png" height="50" hspace="5" vspace="5" style="PADDING-LEFT: 5px; PADDING-RIGHT: 5px;"/></a>
        <a href="https://www.technologyreview.com/s/612114/algorithms-can-turn-any-scene-into-a-comic/"><img src="mit-tech-review.png" height="100" hspace="5" vspace="5" style="PADDING-LEFT: 5px; PADDING-RIGHT: 5px;"/></a>
        <a href="https://www.actualitte.com/article/bd-manga-comics/comixify-transforme-tout-film-en-bande-dessinee-impressionnant/92339"><img src="actualitte.png" height="50" hspace="5" vspace="5" style="PADDING-LEFT: 15px; PADDING-RIGHT: 5px; PADDING-BOTTOM: 15px;"/></a>
        <a href="https://www.chip.pl/2018/12/polacy-zrobili-algorytm-ktory-przeksztalca-film-w-komiks/"><img src="chip.png" height="70" hspace="5" vspace="5" style="PADDING-LEFT: 5px; PADDING-RIGHT: 5px;"/></a>
        <a href="https://www.huffingtonpost.fr/2018/12/17/cette-intelligence-artificielle-transforme-vos-videos-en-bd_a_23620122/"><img src="huffington.png" height="50" hspace="5" vspace="10" style="PADDING-LEFT: 15px; PADDING-RIGHT: 5px; PADDING-BOTTOM: 5px; PADDING-TOP: 20px;"/></a>
        <a href="https://tech.wp.pl/comixify-polscy-naukowcy-stworzyli-algorytm-ktory-przeksztalca-wideo-w-komiks-6329529896867457a"><img src="wp.png" height="60" hspace="5" vspace="0" style="PADDING-LEFT: 15px; PADDING-RIGHT: 5px;"/></a>
        <a href="https://halopolonia.tvp.pl/40709578/03012019-comixify-czyli-przeksztalcanie-wideo-w-komiks"><img src="tvppolonia.png" height="60" hspace="5" vspace="0" style="PADDING-LEFT: 15px; PADDING-RIGHT: 5px;"/></a>
        <a href="https://pytanienasniadanie.tvp.pl/40836129/twoje-ulubione-wideo-jako-komiks"><img src="pns.png" height="60" hspace="5" vspace="0" style="PADDING-LEFT: 15px; PADDING-RIGHT: 5px;"/></a>
        <a href="https://fakty.tvn24.pl/fakty-o-swiecie,61/maciej-gajek-i-tomasz-trzcinski-o-deepfake-ach,952451.html?fbclid=IwAR251a5LaEZp-MGEKPe4IQvPQ2akJuHYrQ6wpG6A52YJak2T3bn7QV96wRI"><img src="tvn24_logo.png" height="60" hspace="5" vspace="0" style="PADDING-LEFT: 15px; PADDING-RIGHT: 5px;"/></a>
        <a href="https://www.sztucznainteligencja.org.pl/jeden-z-dziesieciu-stu-powalcz-o-prestizowy-doktorat/"><img src="sztuczna_inteligencja.png" height="60" hspace="5" vspace="0" style="PADDING-TOP: 15px; PADDING-LEFT: 15px; PADDING-RIGHT: 5px;"/></a>
<a href="https://www.forbes.pl/opinie/czy-postepujaca-automatyzacja-doprowadzi-do-masowych-zwolnien-tomasz-trzcinski/s8mx5nh"><img src="forbes.png" height="60" hspace="5" vspace="0" style="PADDING-LEFT: 15px; PADDING-RIGHT: 5px;"/></a>
      </span>

      <h2>Granty</h2>
        <ul>
          <li><b>PRELUDIUM BIS 4/ST6</b>: Zmierzając w stronę wolumetrycznych rekonstrukcji 3D dla filmów w czasie rzeczywistym, 2023-2027.</li>
          <li><b>OPUS 23/ST6</b>: Dynamiczne sieci neuronowe dla wydajnego uczenia maszynowego, 2023-2027.</li>
          <li><b>PRELUDIUM BIS 3/ST6</b>: Ciągłe uczenie samonadzorowanych reprezentacji, 2022-2026.</li>
          <li><b>OPUS 20/ST6</b>: Głębokie generatywne spojrzenie na uczenie ciągłe, 2021-2024.</li>
	  <li><b>Microsoft Research PhD Scholarship Award</b>: Realistyczne renderowanie postaci ludzi na podstawie niepełnej informacji, 2020-2023.</li>
	  <li><b>Grant Priorytetowe Obszary Badawcze PW - Sztuczna Inteligencja i Robotyka</b>: Binarne reprezentacje danych i ich wykorzystanie w uczeniu ciągłym, 2020-2021.</li>
	  <li><b>Grant Priorytetowe Obszary Badawcze PW - Fizyka Wysokich Energii i Techniki Eksperymentu</b>: WUT@ALICE: Badanie fundamentalnych właściwości silnie oddziałującej materii za pomocą korelacji cząstek oraz uczenia maszynowego w eksperymencie ALICE na LHC, 2020-2021.</li>
	  <li><b>Grant Rady Dyscypliny Naukowej Informatyka Techniczna i Telekomunikacja PW</b>: Opracowanie metody predykcji spontanicznych przedwczesnych porodów na podstawie filmów ultrasonograficznych z wykorzystaniem metod uczenia maszynowego, 2020-2021.</li>
	  <li><b>FNP TEAM-NET (UJ)</b>: Bioinspirowalne sieci neuronowe, 2019-2023. <a href="http://bionn.matinf.uj.edu.pl" target="_blank">strona projektu</a></li>
          <li><b>Google Project ARCore</b>: Hierarchiczna reprezentacja wizualna dla lokalizacji na podstawie obrazu, 2019-2020.</li>
          <li><b>Grant dziekański</b>: Predykcja przedwczesnego porodu w oparciu o zdjęcia ultrasonograficzne przy wykorzystaniu sztucznych sieci neuronowych, 2019.</li>
          <li><b>Google Project ARCore</b>: Poprawa stabilności detekcji punktów charakterystycznych obrazu przy wykorzystaniu głębokich sieci neuronowych, 2018-2019.</li>
          <li><b>Grant dziekański</b>: Zastosowanie głębokich sieci neuronowych do klasyfikacji filmów wideo w sieciach społecznościowych, 2017.</li>
          <li><b>SONATA 11/ST6</b>: Opracowanie metod uczenia maszynowego do monitorowania jakości danych o dużej objętości oraz interaktywnych metod ich wizualizacji na przykładzie eksperymentu ALICE na Wielkim Zderzaczu Hadronów w CERN, 2016-2019.</li>
          <li><b>Google Project Tango</b>: Wydajne i precyzyjne algorytmy wyszukiwania najbliższych sąsiadów dla binarnych deskryptorów lokalnych, 2016-2017.</li>
          <li><b>Grant dziekański</b>: Zastosowanie algorytmów sztucznej inteligencji do analizy zjawiska wirusowych filmów wideo, 2015.</li>
        </ul>

      <h2>Dydaktyka</h2>
        <ul>
          <li><b>Podstawy Sztucznej Inteligencji</b>: PW, od 2017.</li>
          <li><b>Przetwarzanie Cyfrowe Obrazów</b>: PW, od 2015. <a href="POBR_projekt_TT.pdf">informacje projektowe</a></li>
	  <li><b>Analiza Algorytmów</b>: PW, od 2015.</li>
          <li><b>Foundations of Imaging Science</b>: EPFL, 2011-2013.</li>
        </ul>

<a href="prace_dyplomowe.pdf" target="new">Informacje na temat prac dyplomowych (inżynierskich i magisterskich)</a> oraz <a href="https://docs.google.com/document/d/1yB_SIJ01yNCJ9jm1X4Un8qSIRASkYhBvjxdIX6yuVO0/edit?usp=sharing" target="new">aktualna lista tematów</a>

      </div>
    </div>
    <div id="footer">
      Copyright &copy; <a href="http://www.html5webtemplates.co.uk">black_white</a>
    </div>
  </div>
</body>
</html>
